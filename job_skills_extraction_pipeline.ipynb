{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0617b879",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/olohireme/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/olohireme/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/olohireme/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Dense,SpatialDropout1D\n",
    "import contractions\n",
    "import re \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# initializing Stop words libraries\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bbac675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(desc):\n",
    "    desc = contractions.fix(desc)\n",
    "    desc = re.sub(\"[!@.$\\'\\'':()]\", \"\", desc)\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373b6b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_tag(desc):\n",
    "    tokens = nltk.word_tokenize(desc.lower())\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words]\n",
    "    tagged = nltk.pos_tag(filtered_tokens)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70917af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_POS(tagged):\n",
    "    #pattern 1\n",
    "    grammar1 = ('''Noun Phrases: {<DT>?<JJ>*<NN|NNS|NNP>+}''')\n",
    "    chunkParser = nltk.RegexpParser(grammar1)\n",
    "    tree1 = chunkParser.parse(tagged)\n",
    "\n",
    "    # typical noun phrase pattern appending to be concatted later\n",
    "    g1_chunks = []\n",
    "    for subtree in tree1.subtrees(filter=lambda t: t.label() == 'Noun Phrases'):\n",
    "        g1_chunks.append(subtree)\n",
    "    \n",
    "    #pattern 2\n",
    "    grammar2 = ('''NP2: {<IN>?<JJ|NN>*<NNS|NN>} ''')\n",
    "    chunkParser = nltk.RegexpParser(grammar2)\n",
    "    tree2 = chunkParser.parse(tagged)\n",
    "\n",
    "    # variation of a noun phrase pattern to be pickled for later analyses\n",
    "    g2_chunks = []\n",
    "    for subtree in tree2.subtrees(filter=lambda t: t.label() == 'NP2'):\n",
    "        g2_chunks.append(subtree)\n",
    "        \n",
    "    #pattern 3\n",
    "    grammar3 = (''' VS: {<VBG|VBZ|VBP|VBD|VB|VBN><NNS|NN>*}''')\n",
    "    chunkParser = nltk.RegexpParser(grammar3)\n",
    "    tree3 = chunkParser.parse(tagged)\n",
    "\n",
    "    # verb-noun pattern appending to be concatted later\n",
    "    g3_chunks = []\n",
    "    for subtree in tree3.subtrees(filter=lambda t: t.label() == 'VS'):\n",
    "        g3_chunks.append(subtree)\n",
    "        \n",
    "        \n",
    "    # pattern 4\n",
    "    # any number of a singular or plural noun followed by a comma followed by the same noun, noun, noun pattern\n",
    "    grammar4 = ('''Commas: {<NN|NNS>*<,><NN|NNS>*<,><NN|NNS>*} ''')\n",
    "    chunkParser = nltk.RegexpParser(grammar4)\n",
    "    tree4 = chunkParser.parse(tagged)\n",
    "\n",
    "    # common pattern of listing skills appending to be concatted later\n",
    "    g4_chunks = []\n",
    "    for subtree in tree4.subtrees(filter=lambda t: t.label() == 'Commas'):\n",
    "        g4_chunks.append(subtree)\n",
    "        \n",
    "    return g1_chunks, g2_chunks, g3_chunks, g4_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "582bb1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_set(chunks):\n",
    "    '''creates a dataframe that easily parsed with the chunks data '''\n",
    "    df = pd.DataFrame(chunks)    \n",
    "    df.fillna('X', inplace = True)\n",
    "    \n",
    "    train = []\n",
    "    for row in df.values:\n",
    "        phrase = ''\n",
    "        for tup in row:\n",
    "            # needs a space at the end for seperation\n",
    "            phrase += tup[0] + ' '\n",
    "        phrase = ''.join(phrase)\n",
    "        # could use padding tages but encoder method will provide during \n",
    "        # tokenizing/embeddings; X can replace paddding for now\n",
    "        train.append( phrase.replace('X', '').strip())\n",
    "\n",
    "    df['phrase'] = train\n",
    "\n",
    "    #returns 50% of each dataframe to be used if you want to improve execution time\n",
    "    # return df.phrase.sample(frac = 0.5)\n",
    "    # Update: only do 50% if running on excel\n",
    "    return df.phrase\n",
    "\n",
    "def strip_commas(df):\n",
    "    '''create new series of individual n-grams'''\n",
    "    grams = []\n",
    "    for sen in df:\n",
    "        sent = sen.split(',')\n",
    "        for word in sent:\n",
    "            grams.append(word)\n",
    "    return pd.Series(grams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "691a69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrases(desc):\n",
    "    tagged = tokenize_and_tag(desc)\n",
    "    g1_chunks, g2_chunks, g3_chunks, g4_chunks = extract_POS(tagged)\n",
    "    c = training_set(g4_chunks)       \n",
    "    separated_chunks4 = strip_commas(c)\n",
    "    phrases = pd.concat([training_set(g1_chunks),\n",
    "                          training_set(g2_chunks), \n",
    "                          training_set(g3_chunks),\n",
    "                          separated_chunks4], \n",
    "                            ignore_index = True )\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f55f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creates corpus from feature column, which is a pandas series\"\"\"\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for phrase in tqdm(df):\n",
    "        words=[word.lower() for word in word_tokenize(phrase) if(word.isalpha()==1)]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90bb956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create padded sequences of equal lenght as input to LSTM\"\"\"\n",
    "def create_padded_inputs(corpus):\n",
    "    MAX_LEN=20\n",
    "    tokenizer_obj=Tokenizer()\n",
    "    tokenizer_obj.fit_on_texts(corpus)\n",
    "    sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "    phrase_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n",
    "    return phrase_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8317ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(desc):\n",
    "    #clean\n",
    "    desc = clean(desc)\n",
    "    #load model\n",
    "    model = tf.keras.models.load_model('models/lstm_skill_extractor.h5')\n",
    "    #tokenize and convert to phrases\n",
    "    phrases = generate_phrases(desc)\n",
    "    #preprocess unseen data\n",
    "    corpus=create_corpus(phrases)\n",
    "    corpus_pad = create_padded_inputs(corpus)\n",
    "    #get predicted classes\n",
    "    predictions = (model.predict(corpus_pad) > 0.65).astype('int32')\n",
    "    #return predicted skills as list\n",
    "    out = pd.DataFrame({'Phrase':phrases, 'Class':predictions.ravel()})\n",
    "    skills = out.loc[out['Class'] == 1]\n",
    "    return skills['Phrase'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfdce7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with predictions on ketter's repo\n",
    "def get_predictions_excel(filename):\n",
    "    \"\"\"description column must be titled Job Desc\"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Extracted skills'] = df['Job Description'].apply(lambda x: get_predictions(x))\n",
    "    \n",
    "    return df.to_csv('extracted.csv')\n",
    "\n",
    "    #throw error if column name does not exist or file format is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "279766a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-26 23:20:10.303034: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-08-26 23:20:10.303157: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "100%|██████████████████████████████████████| 198/198 [00:00<00:00, 23560.16it/s]\n",
      "2021-08-26 23:20:10.728942: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-08-26 23:20:10.729402: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2021-08-26 23:20:10.802645: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['machine learning',\n",
       " 'member machine',\n",
       " 'various hardware components',\n",
       " 'product managers',\n",
       " 'familiar variety machine learning tools packages',\n",
       " 'various big data tools',\n",
       " 'big data technologies',\n",
       " 'investigate potential algorithmic solutions variety machine',\n",
       " 'technical skills',\n",
       " 'significant development experience python linux',\n",
       " 'code reviews',\n",
       " 'organization skills bachelors',\n",
       " 'computer science',\n",
       " 'equivalent experience bonus qualifications proficiency',\n",
       " 'pytorch background robotics',\n",
       " 'computer vision',\n",
       " 'machine learning',\n",
       " 'remote human guidance create intelligent robots',\n",
       " 'high-performance team role intermediate senior machine',\n",
       " 'member machine',\n",
       " 'various hardware components',\n",
       " 'product managers',\n",
       " 'across north america bring familiar variety machine learning tools',\n",
       " 'algorithms experience various big data tools',\n",
       " 'variety machine',\n",
       " 'technical skills',\n",
       " 'significant development experience python linux reliable production quality software',\n",
       " 'code reviews',\n",
       " 'computer science equivalent experience bonus qualifications',\n",
       " 'pytorch background robotics',\n",
       " 'computer vision',\n",
       " 'learning engineering',\n",
       " 'needs scalability',\n",
       " 'set experiments',\n",
       " 'degree computer science',\n",
       " 'tensorflow pytorch background robotics',\n",
       " ' machine learning ',\n",
       " ' product managers',\n",
       " ' code reviews ',\n",
       " ' organization skills bachelors',\n",
       " 'pytorch background robotics ',\n",
       " ' computer vision ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test get predictions\n",
    "desc = \"\"\"At Kindred, we are on a mission to build human-like intelligence in machines.\n",
    "\n",
    " \n",
    "\n",
    "Since 2014, we've been paving the way for a world filled with more powerful and helpful AI systems. We bring together reinforcement learning, machine learning, and remote human guidance to create intelligent robots that solve real-world problems alongside humans in complex, changing environments like today's supply chain.\n",
    "\n",
    " \n",
    "\n",
    "The company is growing rapidly and we are searching for an individual for a highly-impactful role, has great attention to detail, is a proactive self-starter and interacts easily with all levels of a high-performance team.\n",
    "\n",
    "\n",
    "About this role:\n",
    "\n",
    "As an Intermediate to Senior Machine Learning Engineer, you are a member of our Machine Learning Engineering & SW Platform team. You will design and develop back-end infrastructure to support ML efforts & product features. You will be contributing to design and architecture of complex systems that integrate various hardware components. You will bring up and balance considerations for immediate and future needs for scalability, reliability, security, and robustness. You will closely collaborate with researchers, engineers, and product managers in Toronto and other offices. In this role, you will be part of an on-call rotation in order to service our customers across North America.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "What you bring: \n",
    "\n",
    "Familiar with a variety of Machine Learning tools and packages, understand how to properly set up experiments\n",
    "Good understanding of the mathematical and statistical foundations of ML algorithms\n",
    "Experience with various big data tools and cloud technologies that enable training algorithms at scale\n",
    "Actively stay on top of applied machine learning and big data technologies\n",
    "Desire to investigate potential algorithmic solutions for a variety of machine learning problems\n",
    "Able to tackle varied problems, balancing a long-term mission with short-term requirements\n",
    "\n",
    "Technical Skills: \n",
    "\n",
    "Significant development experience in Python on Linux\n",
    "Reliable production of quality software within time and resource constraints\n",
    "Complete familiarity with modern software development processes such as design documentation, code reviews, CI/CD, testing, project management workflow, and source control conventions\n",
    "Excellent analytical, problem-solving, communication, and organization skills\n",
    "Bachelor's degree in Computer Science or equivalent experience\n",
    "\n",
    "Bonus Qualifications:\n",
    "\n",
    "Proficiency with Golang\n",
    "Experience with deep learning packages such as Tensorflow or PyTorch\n",
    "Background in robotics, computer vision, and deep learning\"\"\"\n",
    "get_predictions(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041dedb",
   "metadata": {},
   "source": [
    "## Rule-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a335051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.19.5'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a204d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.2\n",
      "  latest version: 4.10.3\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/olohireme/miniforge3/envs/mlp\n",
      "\n",
      "  added / updated specs:\n",
      "    - spacy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    brotlipy-0.7.0             |py38hea4295b_1001         340 KB  conda-forge\n",
      "    catalogue-2.0.5            |   py38h10201cd_0          31 KB  conda-forge\n",
      "    charset-normalizer-2.0.0   |     pyhd8ed1ab_0          32 KB  conda-forge\n",
      "    click-7.1.2                |     pyh9f0ad1d_0          64 KB  conda-forge\n",
      "    colorama-0.4.4             |     pyh9f0ad1d_0          18 KB  conda-forge\n",
      "    cymem-2.0.5                |   py38h6f2b01f_2          34 KB  conda-forge\n",
      "    cython-blis-0.7.4          |   py38h8369297_0         1.0 MB  conda-forge\n",
      "    dataclasses-0.8            |     pyhc8e2a94_3          10 KB  conda-forge\n",
      "    idna-3.1                   |     pyhd3deb0d_0          52 KB  conda-forge\n",
      "    murmurhash-1.0.5           |   py38h6f2b01f_0          23 KB  conda-forge\n",
      "    openssl-1.1.1k             |       h3422bc3_1         1.8 MB  conda-forge\n",
      "    pathy-0.6.0                |     pyhd8ed1ab_0          37 KB  conda-forge\n",
      "    preshed-3.0.5              |   py38h6f2b01f_1          88 KB  conda-forge\n",
      "    pydantic-1.8.2             |   py38hea4295b_0         2.0 MB  conda-forge\n",
      "    requests-2.26.0            |     pyhd8ed1ab_0          52 KB  conda-forge\n",
      "    shellingham-1.4.0          |     pyh44b312d_0          11 KB  conda-forge\n",
      "    smart_open-5.2.0           |     pyhd8ed1ab_0          43 KB  conda-forge\n",
      "    spacy-3.1.2                |   py38h1e2c3d9_0         5.2 MB  conda-forge\n",
      "    spacy-legacy-3.0.8         |     pyhd8ed1ab_0          15 KB  conda-forge\n",
      "    srsly-2.4.1                |   py38h6f2b01f_0         519 KB  conda-forge\n",
      "    thinc-8.0.8                |   py38h1e2c3d9_0         634 KB  conda-forge\n",
      "    tqdm-4.62.2                |     pyhd8ed1ab_0          80 KB  conda-forge\n",
      "    typer-0.3.2                |     pyhd8ed1ab_0          23 KB  conda-forge\n",
      "    typing-extensions-3.7.4.3  |                0           8 KB  conda-forge\n",
      "    wasabi-0.8.2               |     pyh44b312d_0          23 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        12.1 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  brotlipy           conda-forge/osx-arm64::brotlipy-0.7.0-py38hea4295b_1001\n",
      "  catalogue          conda-forge/osx-arm64::catalogue-2.0.5-py38h10201cd_0\n",
      "  chardet            conda-forge/osx-arm64::chardet-4.0.0-py38h10201cd_1\n",
      "  charset-normalizer conda-forge/noarch::charset-normalizer-2.0.0-pyhd8ed1ab_0\n",
      "  click              conda-forge/noarch::click-7.1.2-pyh9f0ad1d_0\n",
      "  colorama           conda-forge/noarch::colorama-0.4.4-pyh9f0ad1d_0\n",
      "  cryptography       conda-forge/osx-arm64::cryptography-3.4.7-py38h3c0dae5_0\n",
      "  cymem              conda-forge/osx-arm64::cymem-2.0.5-py38h6f2b01f_2\n",
      "  cython-blis        conda-forge/osx-arm64::cython-blis-0.7.4-py38h8369297_0\n",
      "  dataclasses        conda-forge/noarch::dataclasses-0.8-pyhc8e2a94_3\n",
      "  idna               conda-forge/noarch::idna-3.1-pyhd3deb0d_0\n",
      "  murmurhash         conda-forge/osx-arm64::murmurhash-1.0.5-py38h6f2b01f_0\n",
      "  pathy              conda-forge/noarch::pathy-0.6.0-pyhd8ed1ab_0\n",
      "  preshed            conda-forge/osx-arm64::preshed-3.0.5-py38h6f2b01f_1\n",
      "  pydantic           conda-forge/osx-arm64::pydantic-1.8.2-py38hea4295b_0\n",
      "  pyopenssl          conda-forge/noarch::pyopenssl-20.0.1-pyhd8ed1ab_0\n",
      "  pysocks            conda-forge/osx-arm64::pysocks-1.7.1-py38h10201cd_3\n",
      "  requests           conda-forge/noarch::requests-2.26.0-pyhd8ed1ab_0\n",
      "  shellingham        conda-forge/noarch::shellingham-1.4.0-pyh44b312d_0\n",
      "  smart_open         conda-forge/noarch::smart_open-5.2.0-pyhd8ed1ab_0\n",
      "  spacy              conda-forge/osx-arm64::spacy-3.1.2-py38h1e2c3d9_0\n",
      "  spacy-legacy       conda-forge/noarch::spacy-legacy-3.0.8-pyhd8ed1ab_0\n",
      "  srsly              conda-forge/osx-arm64::srsly-2.4.1-py38h6f2b01f_0\n",
      "  thinc              conda-forge/osx-arm64::thinc-8.0.8-py38h1e2c3d9_0\n",
      "  tqdm               conda-forge/noarch::tqdm-4.62.2-pyhd8ed1ab_0\n",
      "  typer              conda-forge/noarch::typer-0.3.2-pyhd8ed1ab_0\n",
      "  typing-extensions  conda-forge/noarch::typing-extensions-3.7.4.3-0\n",
      "  urllib3            conda-forge/noarch::urllib3-1.26.6-pyhd8ed1ab_0\n",
      "  wasabi             conda-forge/noarch::wasabi-0.8.2-pyh44b312d_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  openssl                                 1.1.1k-h27ca646_0 --> 1.1.1k-h3422bc3_1\n",
      "\n",
      "\n",
      "Proceed ([y]/n)? "
     ]
    }
   ],
   "source": [
    "!conda install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6823e700",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xg/x9dp1h1x32q36fc2kpxyxd180000gn/T/ipykernel_56775/1232691790.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfilter_spans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# These are imported as part of the API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprefer_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_cpu\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/thinc/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigValidationError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormal_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniform_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglorot_uniform_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigure_normal_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoricalCrossentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2Distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCosineDistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceCategoricalCrossentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/thinc/initializers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFloatsXd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mShape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/thinc/backends/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcupy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCupyOps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_cupy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_cupy_allocators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcupy_tensorflow_allocator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcupy_pytorch_allocator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/thinc/backends/cupy_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNumpyOps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_custom_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeviceTypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/mlp/lib/python3.8/site-packages/thinc/backends/numpy_ops.pyx\u001b[0m in \u001b[0;36minit thinc.backends.numpy_ops\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.util import filter_spans\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b3ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7fb490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_left_span(tok, label='', include=True):\n",
    "    offset = 1 if include else 0\n",
    "    idx = tok.i\n",
    "    while idx > tok.left_edge.i:\n",
    "        if tok.doc[idx - 1].pos_ in ('NOUN', 'PROPN', 'ADJ', 'X'):\n",
    "            idx -= 1\n",
    "        else:\n",
    "            break\n",
    "    return label, idx, tok.i+offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e151a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conjugations(tok):\n",
    "    new = [tok]\n",
    "    while new:\n",
    "        tok = new.pop()\n",
    "        yield tok\n",
    "        for child in tok.children:\n",
    "            if child.dep_ == 'conj':\n",
    "                new.append(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cfd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_TERMS = ['experience']\n",
    "def extract_adp_conj_experience(doc, label='EXPERIENCE'):\n",
    "    for tok in doc:\n",
    "        if tok.lower_ in EXP_TERMS:\n",
    "            for child in tok.rights:\n",
    "                if child.dep_ == 'prep':\n",
    "                    for obj in child.children:\n",
    "                        if obj.dep_ == 'pobj':\n",
    "                            for conj in get_conjugations(obj):\n",
    "                                yield get_left_span(conj, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21573ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extractions_2(examples, *extractors):\n",
    "    # Could use context instead of enumerate\n",
    "    doc = nlp(examples, disable=['ner'])\n",
    "    for ent in filter_spans([Span(doc, start, end, label) for extractor in extractors for label, start, end in extractor(doc)]):\n",
    "        sent = ent.root.sent\n",
    "        yield ent.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2487586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_skills(examples, *extractors):\n",
    "    return list(get_extractions_2(examples, *extractors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675282f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_skills(desc, extract_adp_conj_experience))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
