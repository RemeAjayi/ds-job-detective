{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "afcafde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('outputs/data_science_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df77089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[0], axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f7fa545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Job Descriptions\n",
    "desc_df = df['Job Description'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "251ce7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/olohireme/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/olohireme/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "302040b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "174faa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_df['Job Description'] = desc_df['Job Description'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c0b8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_df['Job Description'] = [' '.join(map(str, l)) for l in desc_df['Job Description']]\n",
    "desc_df['Job Description'] = desc_df['Job Description'].str.lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33d31668",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_df['tokenized_desc'] = desc_df['Job Description'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d59a913d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/olohireme/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# initializing Stop words libraries\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d0d229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords \n",
    "desc_df['tokenized_desc'] = desc_df['tokenized_desc'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4674e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Description</th>\n",
       "      <th>tokenized_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>for over 25 years we have been helping our com...</td>\n",
       "      <td>[25, years, helping, community, essential, oil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>about the job individually we are people, but ...</td>\n",
       "      <td>[job, individually, people, ,, together, aviva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>job type: permanent primary location: vancouve...</td>\n",
       "      <td>[job, type, :, permanent, primary, location, :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>our subsidiary is in proptech and real estate ...</td>\n",
       "      <td>[subsidiary, proptech, real, estate, analytics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>who needs insurance? everybody. that keeps us ...</td>\n",
       "      <td>[needs, insurance, ?, everybody, ., keeps, us,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>avant le sras-cov-2, un vaccin qui était dével...</td>\n",
       "      <td>[avant, le, sras-cov-2, ,, un, vaccin, qui, ét...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>responsibilities: develop state-of-the-art com...</td>\n",
       "      <td>[responsibilities, :, develop, state-of-the-ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>qui sommes-nous : buspatrouille est une entrep...</td>\n",
       "      <td>[qui, sommes-nous, :, buspatrouille, est, une,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>for 70 years, charles river employees have wor...</td>\n",
       "      <td>[70, years, ,, charles, river, employees, work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>we are transforming the cyber security industr...</td>\n",
       "      <td>[transforming, cyber, security, industry, us, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Job Description  \\\n",
       "852  for over 25 years we have been helping our com...   \n",
       "207  about the job individually we are people, but ...   \n",
       "627  job type: permanent primary location: vancouve...   \n",
       "671  our subsidiary is in proptech and real estate ...   \n",
       "661  who needs insurance? everybody. that keeps us ...   \n",
       "489  avant le sras-cov-2, un vaccin qui était dével...   \n",
       "434  responsibilities: develop state-of-the-art com...   \n",
       "448  qui sommes-nous : buspatrouille est une entrep...   \n",
       "634  for 70 years, charles river employees have wor...   \n",
       "748  we are transforming the cyber security industr...   \n",
       "\n",
       "                                        tokenized_desc  \n",
       "852  [25, years, helping, community, essential, oil...  \n",
       "207  [job, individually, people, ,, together, aviva...  \n",
       "627  [job, type, :, permanent, primary, location, :...  \n",
       "671  [subsidiary, proptech, real, estate, analytics...  \n",
       "661  [needs, insurance, ?, everybody, ., keeps, us,...  \n",
       "489  [avant, le, sras-cov-2, ,, un, vaccin, qui, ét...  \n",
       "434  [responsibilities, :, develop, state-of-the-ar...  \n",
       "448  [qui, sommes-nous, :, buspatrouille, est, une,...  \n",
       "634  [70, years, ,, charles, river, employees, work...  \n",
       "748  [transforming, cyber, security, industry, us, ...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_df.sample(10, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d165b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_series(keyword):\n",
    "    '''categorizes after tokenizing words with POS tags'''\n",
    "    #tokens = nltk.word_tokenize(keyword)\n",
    "    tagged = nltk.pos_tag(keyword)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb1bba4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 82.1 ms, total: 11.2 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pos_tagged_arrs = desc_df['tokenized_desc'].apply(pos_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bcc5b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unloads the tuples from the tree object for easier manipulation\n",
    "pos_tagged = []\n",
    "for row in pos_tagged_arrs.values:\n",
    "    for element in row:\n",
    "        pos_tagged.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbacfc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe contains all of the words with their corresponding pos tag;\n",
    "pos_df = pd.DataFrame(pos_tagged, columns = ('word','POS'))\n",
    "# special chars were removed due to irrelevance as a tag but will be included in regex\n",
    "char_removal = [',', '.', ':', '#', '$', '\\'\\'', '``', '(', ')']\n",
    "drop_indices = (pos_df.loc[pos_df.POS.isin(char_removal)].index)\n",
    "pos_df.drop(drop_indices, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bcbfb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar1 = ('''Noun Phrases: {<DT>?<JJ>*<NN|NNS|NNP>+}''')\n",
    "chunkParser = nltk.RegexpParser(grammar1)\n",
    "tree1 = chunkParser.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5d5df174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical noun phrase pattern to be pickled for later analyses\n",
    "g1_chunks = []\n",
    "for subtree in tree1.subtrees(filter=lambda t: t.label() == 'Noun Phrases'):\n",
    "    # print(subtree)\n",
    "    g1_chunks.append(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "355c6dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('Noun Phrases', [('metricsflow', 'NN')]),\n",
       " Tree('Noun Phrases', [('work', 'NN'), ('shape', 'NN')]),\n",
       " Tree('Noun Phrases', [('future', 'JJ'), ('data', 'NNS'), ('attribution', 'NN')]),\n",
       " Tree('Noun Phrases', [('new', 'JJ'), ('kind', 'NN'), ('relationship', 'NN'), ('customers', 'NNS'), ('companies', 'NNS')]),\n",
       " Tree('Noun Phrases', [('privacy-first', 'JJ'), ('practices', 'NNS'), ('smarter', 'NN')]),\n",
       " Tree('Noun Phrases', [('non-invasive', 'JJ'), ('customer', 'NN'), ('learning', 'NN')]),\n",
       " Tree('Noun Phrases', [('innovates', 'NNS')]),\n",
       " Tree('Noun Phrases', [('machine', 'NN')]),\n",
       " Tree('Noun Phrases', [('ai', 'JJ'), ('technology', 'NN'), ('equip', 'NN'), ('businesses', 'NNS')]),\n",
       " Tree('Noun Phrases', [('right', 'JJ'), ('tools', 'NNS')]),\n",
       " Tree('Noun Phrases', [('passionate', 'JJ'), ('internet', 'NN'), ('data', 'NNS')]),\n",
       " Tree('Noun Phrases', [('right', 'JJ'), ('place', 'NN')]),\n",
       " Tree('Noun Phrases', [('exceptional', 'JJ'), ('individual', 'JJ'), ('serve', 'NN'), ('data', 'NNS'), ('scientist', 'NN')]),\n",
       " Tree('Noun Phrases', [('successful', 'JJ'), ('candidate', 'NN'), ('work', 'NN')]),\n",
       " Tree('Noun Phrases', [('product', 'NN'), ('engineering', 'NN'), ('team', 'NN'), ('translate', 'NN')]),\n",
       " Tree('Noun Phrases', [('technological', 'JJ'), ('customer', 'NN'), ('requirements', 'NNS')]),\n",
       " Tree('Noun Phrases', [('valuable', 'JJ'), ('features', 'NNS')]),\n",
       " Tree('Noun Phrases', [('great', 'JJ'), ('user', 'JJ'), ('experience', 'NN')]),\n",
       " Tree('Noun Phrases', [('responsibilities', 'NNS')]),\n",
       " Tree('Noun Phrases', [('work', 'NN')])]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1_chunks[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1228cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the scaler to apply it on the test dataset\n",
    "import pickle\n",
    "with open('pickles/chunks_1.pickle', 'wb') as fp1:\n",
    "    pickle.dump(g1_chunks, fp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c5e9781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noun phrase variation \n",
    "# preposition maybe, any number of adjective or nouns, any plural nouns or singular nouns\n",
    "grammar2 = ('''NP2: {<IN>?<JJ|NN>*<NNS|NN>} ''')\n",
    "chunkParser = nltk.RegexpParser(grammar2)\n",
    "tree2 = chunkParser.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ab0143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variation of a noun phrase pattern to be pickled for later analyses\n",
    "g2_chunks = []\n",
    "for subtree in tree2.subtrees(filter=lambda t: t.label() == 'NP2'):\n",
    "    # print(subtree)\n",
    "    g2_chunks.append(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b4e34b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NP2', [('metricsflow', 'NN')]),\n",
       " Tree('NP2', [('work', 'NN'), ('shape', 'NN'), ('future', 'JJ'), ('data', 'NNS')]),\n",
       " Tree('NP2', [('attribution', 'NN'), ('new', 'JJ'), ('kind', 'NN'), ('relationship', 'NN'), ('customers', 'NNS')]),\n",
       " Tree('NP2', [('companies', 'NNS')]),\n",
       " Tree('NP2', [('privacy-first', 'JJ'), ('practices', 'NNS')]),\n",
       " Tree('NP2', [('smarter', 'NN')]),\n",
       " Tree('NP2', [('non-invasive', 'JJ'), ('customer', 'NN'), ('learning', 'NN')]),\n",
       " Tree('NP2', [('innovates', 'NNS')]),\n",
       " Tree('NP2', [('machine', 'NN')]),\n",
       " Tree('NP2', [('ai', 'JJ'), ('technology', 'NN'), ('equip', 'NN'), ('businesses', 'NNS')]),\n",
       " Tree('NP2', [('right', 'JJ'), ('tools', 'NNS')]),\n",
       " Tree('NP2', [('passionate', 'JJ'), ('internet', 'NN'), ('data', 'NNS')]),\n",
       " Tree('NP2', [('right', 'JJ'), ('place', 'NN')]),\n",
       " Tree('NP2', [('exceptional', 'JJ'), ('individual', 'JJ'), ('serve', 'NN'), ('data', 'NNS')]),\n",
       " Tree('NP2', [('scientist', 'NN')]),\n",
       " Tree('NP2', [('successful', 'JJ'), ('candidate', 'NN'), ('work', 'NN')]),\n",
       " Tree('NP2', [('product', 'NN'), ('engineering', 'NN'), ('team', 'NN'), ('translate', 'NN'), ('technological', 'JJ'), ('customer', 'NN'), ('requirements', 'NNS')]),\n",
       " Tree('NP2', [('valuable', 'JJ'), ('features', 'NNS')]),\n",
       " Tree('NP2', [('great', 'JJ'), ('user', 'JJ'), ('experience', 'NN')]),\n",
       " Tree('NP2', [('responsibilities', 'NNS')])]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2_chunks[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a676029",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/chunks_2.pickle', 'wb') as fp2:\n",
    "    pickle.dump(g2_chunks , fp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "efc95c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any sort of verb followed by any number of nouns\n",
    "grammar3 = ('''\n",
    "    VS: {<VBG|VBZ|VBP|VBD|VB|VBN><NNS|NN>*}\n",
    "    ''')\n",
    "chunkParser = nltk.RegexpParser(grammar3)\n",
    "tree3 = chunkParser.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1fd77e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verb-noun pattern to be pickled for later analyses\n",
    "g3_chunks = []\n",
    "for subtree in tree3.subtrees(filter=lambda t: t.label() == 'VS'):\n",
    "    # print(subtree)\n",
    "    g3_chunks.append(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae5fb045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('VS', [('advocating', 'VBG')]),\n",
       " Tree('VS', [('metricsflow', 'VB'), ('innovates', 'NNS')]),\n",
       " Tree('VS', [('using', 'VBG'), ('machine', 'NN')]),\n",
       " Tree('VS', [('learning', 'VBG')]),\n",
       " Tree('VS', [('treated', 'VBD')]),\n",
       " Tree('VS', [('found', 'VBD')]),\n",
       " Tree('VS', [('metricsflow', 'VB')]),\n",
       " Tree('VS', [('seeking', 'VBG')]),\n",
       " Tree('VS', [('ensure', 'VB')]),\n",
       " Tree('VS', [('structured', 'VBD')]),\n",
       " Tree('VS', [('develop', 'VBP')]),\n",
       " Tree('VS', [('uncover', 'VBP')]),\n",
       " Tree('VS', [('drive', 'VBP'), ('projects', 'NNS')]),\n",
       " Tree('VS', [('end', 'VBP'), ('end', 'NN')]),\n",
       " Tree('VS', [('including', 'VBG')]),\n",
       " Tree('VS', [('building', 'VBG'), ('data', 'NNS'), ('pipelines', 'NNS')]),\n",
       " Tree('VS', [('integrating', 'VBG'), ('engineering', 'NN'), ('systems', 'NNS'), ('research', 'NN'), ('industry', 'NN')]),\n",
       " Tree('VS', [('implemented', 'VBD')]),\n",
       " Tree('VS', [('focus', 'VB'), ('data', 'NNS'), ('science', 'NN'), ('features', 'NNS')]),\n",
       " Tree('VS', [('including', 'VBG'), ('data', 'NNS'), ('preparation', 'NN')])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g3_chunks[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1f706000",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/chunks_3.pickle', 'wb') as fp3:\n",
    "    pickle.dump(g3_chunks, fp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "609feda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any number of a singular or plural noun followed by a comma followed by the same noun, noun, noun pattern\n",
    "grammar4 = ('''\n",
    "    Commas: {<NN|NNS>*<,><NN|NNS>*<,><NN|NNS>*} \n",
    "    ''')\n",
    "chunkParser = nltk.RegexpParser(grammar4)\n",
    "tree4 = chunkParser.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "21027934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common pattern of listing skills to be pickled for later analyses\n",
    "g4_chunks = []\n",
    "for subtree in tree4.subtrees(filter=lambda t: t.label() == 'Commas'):\n",
    "    # print(subtree)\n",
    "    g4_chunks.append(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9efce1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('Commas', [('models', 'NNS'), (',', ','), ('metrics', 'NNS'), (',', ','), ('reports', 'NNS')]),\n",
       " Tree('Commas', [('data', 'NNS'), ('preparation', 'NN'), (',', ','), ('machine', 'NN'), ('learning', 'NN'), (',', ',')]),\n",
       " Tree('Commas', [('networks', 'NNS'), (',', ','), ('time-series', 'NNS'), ('analysis', 'NN'), (',', ',')]),\n",
       " Tree('Commas', [('quality', 'NN'), (',', ','), ('safety', 'NN'), (',', ','), ('reliability', 'NN')]),\n",
       " Tree('Commas', [(',', ','), ('deploy', 'NN'), (',', ','), ('maintain', 'NN')]),\n",
       " Tree('Commas', [('production', 'NN'), (',', ','), ('maintenance', 'NN'), (',', ',')]),\n",
       " Tree('Commas', [('years', 'NNS'), ('software', 'NN'), ('development', 'NN'), ('experience', 'NN'), ('experience', 'NN'), ('keras', 'NNS'), (',', ','), ('tensorflow', 'NN'), (',', ','), ('sklearn', 'NN')]),\n",
       " Tree('Commas', [('priorities', 'NNS'), (',', ','), ('requirements', 'NNS'), (',', ','), ('schedules', 'NNS')]),\n",
       " Tree('Commas', [('data', 'NNS'), ('operations', 'NNS'), ('team', 'NN'), (',', ','), ('help', 'NN'), ('design', 'NN'), (',', ','), ('implement', 'NN'), ('maintain', 'NN'), ('batch', 'NN'), ('real-time', 'NN')]),\n",
       " Tree('Commas', [('addition', 'NN'), (',', ','), ('support', 'NN'), ('software', 'NN'), ('developers', 'NNS'), (',', ','), ('database', 'NN'), ('architects', 'NNS')]),\n",
       " Tree('Commas', [('products', 'NNS'), ('services', 'NNS'), ('design', 'NN'), (',', ','), ('implement', 'NN'), (',', ',')]),\n",
       " Tree('Commas', [('sla', 'NNS'), (',', ','), ('performance', 'NN'), ('measurements', 'NNS'), (',', ',')]),\n",
       " Tree('Commas', [('safety', 'NN'), ('aggregate', 'NN'), ('reports', 'NNS'), (',', ','), ('literature', 'NN'), ('surveillance', 'NN'), (',', ',')]),\n",
       " Tree('Commas', [('projects', 'NNS'), (',', ','), ('enter', 'NN'), (',', ',')]),\n",
       " Tree('Commas', [('metrics', 'NNS'), ('management', 'NN'), (',', ','), ('reconciliation', 'NN'), (',', ','), ('audit', 'NN'), ('training', 'NN'), ('tasks', 'NNS')]),\n",
       " Tree('Commas', [('e.g.', 'NN'), (',', ','), ('word', 'NN'), (',', ','), ('excel', 'NN')]),\n",
       " Tree('Commas', [('business', 'NN'), ('strategy', 'NN'), (',', ','), ('cloud', 'NN'), ('engineering', 'NN'), (',', ','), ('client', 'NN'), ('engineering', 'NN')]),\n",
       " Tree('Commas', [('data', 'NNS'), ('ingestion', 'NN'), (',', ','), ('data', 'NNS'), ('modelling', 'NN'), (',', ','), ('data', 'NNS'), ('processing', 'NN'), ('data', 'NNS'), ('governance', 'NN')]),\n",
       " Tree('Commas', [('bash', 'NN'), (',', ','), ('python', 'NN'), (',', ',')]),\n",
       " Tree('Commas', [('cli', 'NN'), (',', ','), ('lambda', 'NN'), (',', ','), ('kinesis', 'NN'), ('firehose', 'NN')])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g4_chunks[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a8c09144",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickles/chunks_4.pickle', 'wb') as fp4:\n",
    "    pickle.dump(g4_chunks, fp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dba8a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded at the bottom of this page to inspect as needed\n",
    "chunks1 = pickle.load( open('pickles/chunks_1.pickle', \"rb\" ) )\n",
    "chunks2 = pickle.load( open('pickles/chunks_2.pickle', \"rb\" ) )\n",
    "chunks3 = pickle.load( open('pickles/chunks_3.pickle', \"rb\" ) )\n",
    "chunks4 = pickle.load( open('pickles/chunks_4.pickle', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "683750a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 125438 Sample Size: 12543.800000000001\n",
      "Length: 136372 Sample Size: 13637.2\n",
      "Length: 59342 Sample Size: 5934.200000000001\n",
      "Length: 8099 Sample Size: 809.9000000000001\n"
     ]
    }
   ],
   "source": [
    "print('Length:', len(chunks1), 'Sample Size:', len(chunks1) * .10)\n",
    "print('Length:', len(chunks2), 'Sample Size:', len(chunks2) * .10) \n",
    "print('Length:', len(chunks3), 'Sample Size:', len(chunks3) * .10)\n",
    "print('Length:', len(chunks4), 'Sample Size:', len(chunks4) * .10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5008fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
